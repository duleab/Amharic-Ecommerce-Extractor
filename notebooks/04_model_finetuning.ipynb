{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Amharic NER Model Fine-tuning\n",
        "\n",
        "This notebook demonstrates how to fine-tune a transformer model for Amharic Named Entity Recognition (NER).\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Load the labeled data from the previous step\n",
        "2. Prepare the data for training, validation, and testing\n",
        "3. Fine-tune a pre-trained transformer model for Amharic NER\n",
        "4. Evaluate the model's performance\n",
        "5. Save the fine-tuned model for further use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add the project root directory to the Python path\n",
        "project_root = Path().resolve().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "# Import the AmharicNERTrainer class from our custom module\n",
        "from src.models.model_trainer import AmharicNERTrainer\n",
        "\n",
        "# Check if GPU is available\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define input and output directories\n",
        "labeled_data_dir = project_root / \"data\" / \"labeled\"\n",
        "models_dir = project_root / \"data\" / \"models\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Labeled data directory: {labeled_data_dir}\")\n",
        "print(f\"Models directory: {models_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the labeled data\n",
        "def load_labeled_data():\n",
        "    \"\"\"\n",
        "    Load the labeled data from the CoNLL file.\n",
        "    \n",
        "    Returns:\n",
        "        List of examples with tokens and labels\n",
        "    \"\"\"\n",
        "    conll_path = labeled_data_dir / \"labeled_data.conll\"\n",
        "    \n",
        "    if not conll_path.exists():\n",
        "        print(f\"Labeled data not found at {conll_path}\")\n",
        "        return None\n",
        "    \n",
        "    # Initialize the trainer\n",
        "    trainer = AmharicNERTrainer(\n",
        "        model_name=\"xlm-roberta-base\",\n",
        "        output_dir=str(models_dir)\n",
        "    )\n",
        "    \n",
        "    # Load the labeled data\n",
        "    examples = trainer.load_conll_data(conll_path)\n",
        "    \n",
        "    print(f\"Loaded {len(examples)} examples from {conll_path}\")\n",
        "    \n",
        "    # Display a sample example\n",
        "    if examples:\n",
        "        print(\"\\nSample example:\")\n",
        "        sample = examples[0]\n",
        "        print(f\"Tokens: {sample['tokens'][:10]}...\")\n",
        "        print(f\"Labels: {sample['labels'][:10]}...\")\n",
        "    \n",
        "    return examples, trainer\n",
        "\n",
        "# Load the labeled data\n",
        "# Note: This will only work after manual labeling is complete\n",
        "# examples, trainer = load_labeled_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the data for training\n",
        "def prepare_data(examples, trainer, test_size=0.2, val_size=0.1):\n",
        "    \"\"\"\n",
        "    Prepare the data for training, validation, and testing.\n",
        "    \n",
        "    Args:\n",
        "        examples: List of examples with tokens and labels\n",
        "        trainer: AmharicNERTrainer instance\n",
        "        test_size: Proportion of data to use for testing\n",
        "        val_size: Proportion of data to use for validation\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with train, validation, and test datasets\n",
        "    \"\"\"\n",
        "    if not examples:\n",
        "        print(\"No examples to prepare\")\n",
        "        return None\n",
        "    \n",
        "    # Prepare the datasets\n",
        "    datasets = trainer.prepare_dataset(examples, test_size=test_size, val_size=val_size)\n",
        "    \n",
        "    print(f\"Prepared datasets:\")\n",
        "    print(f\"- Training: {len(datasets['train'])} examples\")\n",
        "    print(f\"- Validation: {len(datasets['validation'])} examples\")\n",
        "    print(f\"- Testing: {len(datasets['test'])} examples\")\n",
        "    \n",
        "    return datasets\n",
        "\n",
        "# Prepare the data for training\n",
        "# datasets = prepare_data(examples, trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "def train_model(datasets, trainer, learning_rate=2e-5, batch_size=16, num_epochs=3):\n",
        "    \"\"\"\n",
        "    Fine-tune the model on the prepared datasets.\n",
        "    \n",
        "    Args:\n",
        "        datasets: Dictionary with train, validation, and test datasets\n",
        "        trainer: AmharicNERTrainer instance\n",
        "        learning_rate: Learning rate for training\n",
        "        batch_size: Batch size for training\n",
        "        num_epochs: Number of epochs to train for\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with evaluation metrics\n",
        "    \"\"\"\n",
        "    if not datasets:\n",
        "        print(\"No datasets to train on\")\n",
        "        return None\n",
        "    \n",
        "    print(\"Starting model fine-tuning...\")\n",
        "    print(f\"- Learning rate: {learning_rate}\")\n",
        "    print(f\"- Batch size: {batch_size}\")\n",
        "    print(f\"- Number of epochs: {num_epochs}\")\n",
        "    \n",
        "    # Train the model\n",
        "    test_results = trainer.train(\n",
        "        datasets,\n",
        "        learning_rate=learning_rate,\n",
        "        batch_size=batch_size,\n",
        "        num_epochs=num_epochs\n",
        "    )\n",
        "    \n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(\"\\nTest results:\")\n",
        "    for metric, value in test_results.items():\n",
        "        print(f\"- {metric}: {value:.4f}\")\n",
        "    \n",
        "    return test_results\n",
        "\n",
        "# Fine-tune the model\n",
        "# Note: This will take some time to run, especially without a GPU\n",
        "# test_results = train_model(datasets, trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different models\n",
        "def compare_models():\n",
        "    \"\"\"\n",
        "    Compare different transformer models for Amharic NER.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with model names and their test results\n",
        "    \"\"\"\n",
        "    # Define the models to compare\n",
        "    models = [\n",
        "        \"xlm-roberta-base\",\n",
        "        \"bert-base-multilingual-cased\",\n",
        "        \"distilbert-base-multilingual-cased\",\n",
        "        # Add more models here\n",
        "    ]\n",
        "    \n",
        "    # Load the labeled data\n",
        "    conll_path = labeled_data_dir / \"labeled_data.conll\"\n",
        "    \n",
        "    if not conll_path.exists():\n",
        "        print(f\"Labeled data not found at {conll_path}\")\n",
        "        return None\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for model_name in models:\n",
        "        print(f\"\\n{'=' * 50}\")\n",
        "        print(f\"Training model: {model_name}\")\n",
        "        print(f\"{'=' * 50}\")\n",
        "        \n",
        "        # Initialize the trainer\n",
        "        trainer = AmharicNERTrainer(\n",
        "            model_name=model_name,\n",
        "            output_dir=str(models_dir / model_name.replace(\"/\", \"-\"))\n",
        "        )\n",
        "        \n",
        "        # Load the labeled data\n",
        "        examples = trainer.load_conll_data(conll_path)\n",
        "        \n",
        "        # Prepare the datasets\n",
        "        datasets = trainer.prepare_dataset(examples)\n",
        "        \n",
        "        # Train the model\n",
        "        test_results = trainer.train(\n",
        "            datasets,\n",
        "            learning_rate=2e-5,\n",
        "            batch_size=16,\n",
        "            num_epochs=3\n",
        "        )\n",
        "        \n",
        "        # Store the results\n",
        "        results[model_name] = test_results\n",
        "        \n",
        "        print(f\"\\nResults for {model_name}:\")\n",
        "        for metric, value in test_results.items():\n",
        "            print(f\"- {metric}: {value:.4f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Compare different models\n",
        "# Note: This will take a long time to run\n",
        "# model_comparison = compare_models()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison results\n",
        "def visualize_results(results):\n",
        "    \"\"\"\n",
        "    Visualize the results of the model comparison.\n",
        "    \n",
        "    Args:\n",
        "        results: Dictionary with model names and their test results\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        print(\"No results to visualize\")\n",
        "        return\n",
        "    \n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    \n",
        "    # Extract metrics\n",
        "    metrics = list(next(iter(results.values())).keys())\n",
        "    \n",
        "    # Create a DataFrame with the results\n",
        "    df_results = pd.DataFrame(index=results.keys(), columns=metrics)\n",
        "    \n",
        "    for model_name, model_results in results.items():\n",
        "        for metric, value in model_results.items():\n",
        "            df_results.loc[model_name, metric] = value\n",
        "    \n",
        "    # Display the results\n",
        "    print(\"Model comparison results:\")\n",
        "    display(df_results)\n",
        "    \n",
        "    # Plot the results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    for i, metric in enumerate(metrics):\n",
        "        plt.subplot(1, len(metrics), i+1)\n",
        "        sns.barplot(x=df_results.index, y=df_results[metric])\n",
        "        plt.title(f\"{metric}\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.ylim(0, 1)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the model comparison results\n",
        "# visualize_results(model_comparison)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "In this notebook, we have:\n",
        "\n",
        "1. Set up the environment for fine-tuning transformer models\n",
        "2. Created functions to load and prepare labeled data\n",
        "3. Implemented a function to fine-tune a single model\n",
        "4. Developed a framework for comparing different models\n",
        "5. Added visualization tools for model comparison results\n",
        "\n",
        "To use this notebook:\n",
        "\n",
        "1. Complete the manual labeling in the previous notebook\n",
        "2. Run the `load_labeled_data()` function to load the labeled data\n",
        "3. Run the `prepare_data()` function to prepare the data for training\n",
        "4. Run the `train_model()` function to fine-tune a single model\n",
        "5. Alternatively, run the `compare_models()` function to compare multiple models\n",
        "6. Use the `visualize_results()` function to visualize the comparison results\n",
        "\n",
        "In the next notebook, we will explore model interpretability using SHAP and LIME to understand how our fine-tuned model makes predictions.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
