{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 5. Model Evaluation\n",
        "\n",
        "This notebook evaluates and compares different fine-tuned NER models for Amharic e-commerce data extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(\"..\")\n",
        "from src.models.model_evaluator import AmharicNEREvaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 5.1 Load Test Data\n",
        "\n",
        "First, we need to load the test data that will be used to evaluate our models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "data_dir = Path(\"../data\")\n",
        "test_data_path = data_dir / \"labeled\" / \"test_data.json\"\n",
        "models_dir = data_dir / \"models\"\n",
        "output_dir = data_dir / \"evaluations\"\n",
        "\n",
        "# Create output directory if it does not exist\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Check if test data exists\n",
        "if not test_data_path.exists():\n",
        "    print(f\"Test data not found at {test_data_path}\")\n",
        "    print(\"You need to create test data first by running the data labeling notebook.\")\n",
        "else:\n",
        "    # Load test data\n",
        "    with open(test_data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        test_data = json.load(f)\n",
        "    print(f\"Loaded {len(test_data)} test examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 5.2 Initialize Evaluator\n",
        "\n",
        "Now we will initialize the model evaluator that will compare our different models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluator\n",
        "evaluator = AmharicNEREvaluator(\n",
        "    models_dir=models_dir,\n",
        "    output_dir=output_dir\n",
        ")\n",
        "\n",
        "# List available models\n",
        "print(\"Available models:\")\n",
        "for model_path in evaluator.model_paths:\n",
        "    print(f\"- {model_path.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 5.3 Evaluate Individual Models\n",
        "\n",
        "Let us evaluate each model individually to see how they perform.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate each model\n",
        "model_metrics = {}\n",
        "\n",
        "for model_path in evaluator.model_paths:\n",
        "    print(f\"Evaluating model: {model_path.name}\")\n",
        "    metrics = evaluator.evaluate_model(model_path, test_data)\n",
        "    model_metrics[model_path.name] = metrics\n",
        "    \n",
        "    # Print overall metrics\n",
        "    if metrics and \"overall\" in metrics:\n",
        "        print(f\"Precision: {metrics['overall']['precision']:.4f}\")\n",
        "        print(f\"Recall: {metrics['overall']['recall']:.4f}\")\n",
        "        print(f\"F1 Score: {metrics['overall']['f1']:.4f}\")\n",
        "        print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 5.4 Compare Models\n",
        "\n",
        "Now let us compare all models side by side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract overall metrics for each model\n",
        "models = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for model_name, metrics in model_metrics.items():\n",
        "    if metrics and \"overall\" in metrics:\n",
        "        models.append(model_name)\n",
        "        precisions.append(metrics['overall']['precision'])\n",
        "        recalls.append(metrics['overall']['recall'])\n",
        "        f1_scores.append(metrics['overall']['f1'])\n",
        "\n",
        "# Create DataFrame for comparison\n",
        "comparison_df = pd.DataFrame({\n",
        "    \"Model\": models,\n",
        "    \"Precision\": precisions,\n",
        "    \"Recall\": recalls,\n",
        "    \"F1 Score\": f1_scores\n",
        "})\n",
        "\n",
        "# Sort by F1 score\n",
        "comparison_df = comparison_df.sort_values(\"F1 Score\", ascending=False)\n",
        "\n",
        "# Display comparison\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 5.4 Compare Models\n",
        "\n",
        "Now let us compare all models side by side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 5. Model Evaluation\n",
        "\n",
        "This notebook evaluates and compares different fine-tuned NER models for Amharic e-commerce data extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(\"..\")\n",
        "from src.models.model_evaluator import AmharicNEREvaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5.1 Load Test Data\n",
        "\n",
        "First, we need to load the test data that will be used to evaluate our models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths\n",
        "data_dir = Path(\"../data\")\n",
        "test_data_path = data_dir / \"labeled\" / \"test_data.json\"\n",
        "models_dir = data_dir / \"models\"\n",
        "output_dir = data_dir / \"evaluations\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Check if test data exists\n",
        "if not test_data_path.exists():\n",
        "    print(f\"Test data not found at {test_data_path}\")\n",
        "    print(\"You need to create test data first by running the data labeling notebook.\")\n",
        "else:\n",
        "    # Load test data\n",
        "    with open(test_data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        test_data = json.load(f)\n",
        "    print(f\"Loaded {len(test_data)} test examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5.2 Initialize Evaluator\n",
        "\n",
        "Now we'll initialize the model evaluator that will compare our different models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluator\n",
        "evaluator = AmharicNEREvaluator(\n",
        "    models_dir=models_dir,\n",
        "    output_dir=output_dir\n",
        ")\n",
        "\n",
        "# List available models\n",
        "print(\"Available models:\")\n",
        "for model_path in evaluator.model_paths:\n",
        "    print(f\"- {model_path.name}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
