{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Amharic E-commerce Data Labeling\n",
        "\n",
        "This notebook demonstrates how to label Amharic e-commerce data for Named Entity Recognition (NER) tasks.\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Load the preprocessed data\n",
        "2. Convert the data to CoNLL format for labeling\n",
        "3. Provide a labeling interface for manual annotation\n",
        "4. Validate the labeled data\n",
        "5. Generate statistics on the labeled entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Add the project root directory to the Python path\n",
        "project_root = Path().resolve().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "# Import the NERLabeler class from our custom module\n",
        "from src.data.labeling_utils import NERLabeler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed data directory: D:\\10-Academy\\Week4\\amharic-ecommerce-extractor\\data\\processed\n",
            "Labeled data directory: D:\\10-Academy\\Week4\\amharic-ecommerce-extractor\\data\\labeled\n"
          ]
        }
      ],
      "source": [
        "# Initialize the NERLabeler\n",
        "labeler = NERLabeler()\n",
        "\n",
        "# Define input and output directories\n",
        "processed_data_dir = project_root / \"data\" / \"processed\"\n",
        "labeled_data_dir = project_root / \"data\" / \"labeled\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(labeled_data_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Processed data directory: {processed_data_dir}\")\n",
        "print(f\"Labeled data directory: {labeled_data_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 58875 tokens from D:\\10-Academy\\Week4\\amharic-ecommerce-extractor\\data\\processed\\ner_ready_data.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message_id</th>\n",
              "      <th>channel</th>\n",
              "      <th>token</th>\n",
              "      <th>entity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>188877.0</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>üì¢</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>188877.0</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·ã≠·àÖ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>188877.0</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·ã®·â≤·ä≠·â´·àÖ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>188877.0</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·â¢·ãù·äê·àµ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>188877.0</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·â§·â∞·à∞·â•</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>188877.0</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·â§·âµ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>188877.0</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·äê·ãç</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>188877.0</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·ç£</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>188877.0</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·à∞·ãç·äï</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>188877.0</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·ä•·äï·ã≤·àÅ·àù</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   message_id         channel  token entity\n",
              "0    188877.0  @tikvahethmart      üì¢      O\n",
              "1    188877.0  @tikvahethmart     ·ã≠·àÖ      O\n",
              "2    188877.0  @tikvahethmart  ·ã®·â≤·ä≠·â´·àÖ      O\n",
              "3    188877.0  @tikvahethmart   ·â¢·ãù·äê·àµ      O\n",
              "4    188877.0  @tikvahethmart   ·â§·â∞·à∞·â•      O\n",
              "5    188877.0  @tikvahethmart     ·â§·âµ      O\n",
              "6    188877.0  @tikvahethmart     ·äê·ãç      O\n",
              "7    188877.0  @tikvahethmart      ·ç£      O\n",
              "8    188877.0  @tikvahethmart    ·à∞·ãç·äï      O\n",
              "9    188877.0  @tikvahethmart  ·ä•·äï·ã≤·àÅ·àù      O"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the preprocessed data\n",
        "def load_ner_ready_data():\n",
        "    \"\"\"\n",
        "    Load the NER-ready data from the processed data directory.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with token and entity columns\n",
        "    \"\"\"\n",
        "    ner_data_path = processed_data_dir / \"ner_ready_data.csv\"\n",
        "    \n",
        "    if not ner_data_path.exists():\n",
        "        print(f\"NER-ready data not found at {ner_data_path}\")\n",
        "        return None\n",
        "    \n",
        "    df = pd.read_csv(ner_data_path)\n",
        "    print(f\"Loaded {len(df)} tokens from {ner_data_path}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load the NER-ready data\n",
        "ner_data = load_ner_ready_data()\n",
        "\n",
        "# Display the first few rows\n",
        "if ner_data is not None:\n",
        "    display(ner_data.head(10))\n",
        "else:\n",
        "    print(\"No data to display\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-22 12:51:36,079 - src.data.labeling_utils - ERROR - Error converting CSV to CoNLL: argument of type 'method' is not iterable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted data to CoNLL format and saved to D:\\10-Academy\\Week4\\amharic-ecommerce-extractor\\data\\labeled\\unlabeled_data.conll\n"
          ]
        }
      ],
      "source": [
        "# Convert to CoNLL format for labeling\n",
        "def convert_to_conll():\n",
        "    \"\"\"\n",
        "    Convert the NER-ready data to CoNLL format for labeling.\n",
        "    \n",
        "    Returns:\n",
        "        Path to the CoNLL file\n",
        "    \"\"\"\n",
        "    if ner_data is None:\n",
        "        print(\"No data to convert\")\n",
        "        return None\n",
        "    \n",
        "    # Define the path for the CoNLL file\n",
        "    conll_path = labeled_data_dir / \"unlabeled_data.conll\"\n",
        "    \n",
        "    # Convert to CoNLL format\n",
        "    conll_text = labeler.csv_to_conll(ner_data, conll_path)\n",
        "    \n",
        "    print(f\"Converted data to CoNLL format and saved to {conll_path}\")\n",
        "    \n",
        "    return conll_path\n",
        "\n",
        "# Convert to CoNLL format\n",
        "conll_path = convert_to_conll()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual Labeling Instructions\n",
        "\n",
        "To label the data for NER, follow these steps:\n",
        "\n",
        "1. Open the CoNLL file in a text editor\n",
        "2. For each token, replace the \"O\" label with the appropriate entity label:\n",
        "   - B-Product: Beginning of a product entity\n",
        "   - I-Product: Inside a product entity\n",
        "   - B-PRICE: Beginning of a price entity\n",
        "   - I-PRICE: Inside a price entity\n",
        "   - B-LOC: Beginning of a location entity\n",
        "   - I-LOC: Inside a location entity\n",
        "   - B-DELIVERY_FEE: Beginning of a delivery fee entity\n",
        "   - I-DELIVERY_FEE: Inside a delivery fee entity\n",
        "   - B-CONTACT_INFO: Beginning of a contact info entity\n",
        "   - I-CONTACT_INFO: Inside a contact info entity\n",
        "   - O: Not an entity (Outside)\n",
        "\n",
        "3. Save the labeled file as \"labeled_data.conll\" in the labeled data directory\n",
        "\n",
        "### Example:\n",
        "\n",
        "```\n",
        "·å•·à© O\n",
        "·à±·çê·à≠ B-Product\n",
        "·àõ·à≠·ä¨·âµ I-Product\n",
        "·ãç·àµ·å• O\n",
        "·ã´·àâ O\n",
        "·àÅ·àâ·àù O\n",
        "·ä†·ã≠·äê·âµ O\n",
        "·ã®·àÖ·çÉ·äì·âµ B-Product\n",
        "·àù·åç·â¶·âΩ I-Product\n",
        "·â† O\n",
        "·ãã·åã O\n",
        "·âÖ·äì·àΩ O\n",
        "·ã≠·âÄ·à≠·â£·àç O\n",
        "·ç°·ç° O\n",
        "·ã®·àÖ·çÉ·äì·âµ B-Product\n",
        "·ãà·â∞·âµ I-Product\n",
        "·â† O\n",
        "250 B-PRICE\n",
        "·â•·à≠ I-PRICE\n",
        "·â•·âª O\n",
        "·ç°·ç° O\n",
        "·â¶·àå B-LOC\n",
        "·ä†·ä´·â£·â¢ I-LOC\n",
        "·äê·ãç O\n",
        "·ç°·ç° O\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate the labeled data\n",
        "def validate_labeled_data():\n",
        "    \"\"\"\n",
        "    Validate the labeled data for common errors.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (is_valid, errors)\n",
        "    \"\"\"\n",
        "    labeled_conll_path = labeled_data_dir / \"labeled_data.conll\"\n",
        "    \n",
        "    if not labeled_conll_path.exists():\n",
        "        print(f\"Labeled data not found at {labeled_conll_path}\")\n",
        "        return False, [\"File not found\"]\n",
        "    \n",
        "    # Validate the CoNLL file\n",
        "    is_valid, errors = labeler.validate_conll(labeled_conll_path)\n",
        "    \n",
        "    if is_valid:\n",
        "        print(\"Labeled data is valid!\")\n",
        "    else:\n",
        "        print(f\"Found {len(errors)} errors in the labeled data:\")\n",
        "        for error in errors:\n",
        "            print(f\"- {error}\")\n",
        "    \n",
        "    return is_valid, errors\n",
        "\n",
        "# This function should be run after manual labeling is complete\n",
        "# validate_labeled_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate statistics on labeled data\n",
        "def generate_label_statistics():\n",
        "    \"\"\"\n",
        "    Generate statistics on the labeled data.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with statistics\n",
        "    \"\"\"\n",
        "    labeled_conll_path = labeled_data_dir / \"labeled_data.conll\"\n",
        "    \n",
        "    if not labeled_conll_path.exists():\n",
        "        print(f\"Labeled data not found at {labeled_conll_path}\")\n",
        "        return None\n",
        "    \n",
        "    # Generate statistics\n",
        "    statistics = labeler.generate_statistics(labeled_conll_path)\n",
        "    \n",
        "    # Display statistics\n",
        "    print(f\"Total tokens: {statistics['total_tokens']}\")\n",
        "    print(f\"Total sentences: {statistics['total_sentences']}\")\n",
        "    \n",
        "    print(\"\\nEntity token counts:\")\n",
        "    for entity, count in statistics['entity_token_counts'].items():\n",
        "        if count > 0:\n",
        "            print(f\"- {entity}: {count}\")\n",
        "    \n",
        "    print(\"\\nEntity span counts:\")\n",
        "    for entity, count in statistics['entity_span_counts'].items():\n",
        "        print(f\"- {entity}: {count}\")\n",
        "    \n",
        "    print(\"\\nEntity examples:\")\n",
        "    for entity, examples in statistics['entity_examples'].items():\n",
        "        if examples:\n",
        "            print(f\"\\n{entity} examples:\")\n",
        "            for example in examples:\n",
        "                print(f\"- {example}\")\n",
        "    \n",
        "    return statistics\n",
        "\n",
        "# This function should be run after manual labeling is complete\n",
        "# statistics = generate_label_statistics()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert labeled data to CSV format\n",
        "def convert_to_csv():\n",
        "    \"\"\"\n",
        "    Convert the labeled CoNLL data back to CSV format.\n",
        "    \n",
        "    Returns:\n",
        "        Path to the CSV file\n",
        "    \"\"\"\n",
        "    labeled_conll_path = labeled_data_dir / \"labeled_data.conll\"\n",
        "    \n",
        "    if not labeled_conll_path.exists():\n",
        "        print(f\"Labeled data not found at {labeled_conll_path}\")\n",
        "        return None\n",
        "    \n",
        "    # Define the path for the CSV file\n",
        "    csv_path = labeled_data_dir / \"labeled_data.csv\"\n",
        "    \n",
        "    # Convert to CSV format\n",
        "    df = labeler.conll_to_csv(labeled_conll_path, csv_path)\n",
        "    \n",
        "    print(f\"Converted labeled data to CSV format and saved to {csv_path}\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    \n",
        "    # Display the first few rows\n",
        "    display(df.head(10))\n",
        "    \n",
        "    return csv_path\n",
        "\n",
        "# This function should be run after manual labeling is complete\n",
        "# csv_path = convert_to_csv()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "In this notebook, we have:\n",
        "\n",
        "1. Loaded the preprocessed data from the previous step\n",
        "2. Converted the data to CoNLL format for labeling\n",
        "3. Provided instructions for manual annotation\n",
        "4. Created functions to validate the labeled data\n",
        "5. Implemented tools to generate statistics on the labeled entities\n",
        "6. Prepared the labeled data for model fine-tuning\n",
        "\n",
        "After completing the manual labeling:\n",
        "\n",
        "1. Run the `validate_labeled_data()` function to check for errors\n",
        "2. Run the `generate_label_statistics()` function to analyze the labeled entities\n",
        "3. Run the `convert_to_csv()` function to prepare the data for the next step\n",
        "\n",
        "In the next notebook, we will fine-tune a transformer model for Amharic NER using this labeled data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
