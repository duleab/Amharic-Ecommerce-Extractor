{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Amharic E-commerce Data Preprocessing\n",
        "\n",
        "This notebook demonstrates how to preprocess Amharic e-commerce data for Named Entity Recognition (NER) tasks.\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Load the raw data collected from Telegram channels\n",
        "2. Clean and normalize the Amharic text\n",
        "3. Extract potential entities using pattern matching\n",
        "4. Prepare the data for NER labeling\n",
        "5. Save the processed data for the next step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# This gets the notebook's current working directory and goes two levels up to project root\n",
        "project_root = Path.cwd().parent.parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "# Now your import will work\n",
        "from src.data.preprocessor import AmharicPreprocessor\n",
        "\n",
        "# You can initialize and use the preprocessor now\n",
        "preprocessor = AmharicPreprocessor()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "# Add the project root directory to the Python path\n",
        "project_root = Path().resolve().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "# Import the AmharicPreprocessor class from our custom module\n",
        "from src.data.preprocessor import AmharicPreprocessor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw data directory: D:\\10-Academy\\Week4\\amharic-ecommerce-extractor\\data\\raw\n",
            "Processed data directory: D:\\10-Academy\\Week4\\amharic-ecommerce-extractor\\data\\processed\n"
          ]
        }
      ],
      "source": [
        "# Initialize the AmharicPreprocessor\n",
        "preprocessor = AmharicPreprocessor()\n",
        "\n",
        "# Define input and output directories\n",
        "raw_data_dir = project_root / \"data\" / \"raw\"\n",
        "processed_data_dir = project_root / \"data\" / \"processed\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(processed_data_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Raw data directory: {raw_data_dir}\")\n",
        "print(f\"Processed data directory: {processed_data_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from D:\\10-Academy\\Week4\\amharic-ecommerce-extractor\\data\\raw\\all_messages_20250622_102341.json\n",
            "Loaded 1045 messages\n"
          ]
        }
      ],
      "source": [
        "# Function to load data from raw files\n",
        "def load_raw_data(file_path=None):\n",
        "    \"\"\"\n",
        "    Load raw data from JSON or CSV file.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to the data file (optional)\n",
        "        \n",
        "    Returns:\n",
        "        List of dictionaries containing message data\n",
        "    \"\"\"\n",
        "    if file_path is None:\n",
        "        # Find the most recent JSON file\n",
        "        json_files = list(raw_data_dir.glob(\"all_messages_*.json\"))\n",
        "        csv_files = list(raw_data_dir.glob(\"telegram_data_*.csv\"))\n",
        "        \n",
        "        if json_files:\n",
        "            # Use the most recent JSON file\n",
        "            file_path = sorted(json_files)[-1]\n",
        "        elif csv_files:\n",
        "            # Use the most recent CSV file\n",
        "            file_path = sorted(csv_files)[-1]\n",
        "        else:\n",
        "            print(\"No data files found in the raw data directory\")\n",
        "            return []\n",
        "    \n",
        "    print(f\"Loading data from {file_path}\")\n",
        "    data = preprocessor.load_data(file_path)\n",
        "    print(f\"Loaded {len(data)} messages\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Load raw data\n",
        "raw_data = load_raw_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of messages: 1045\n",
            "Columns: ['id', 'date', 'text', 'views', 'channel', 'has_media', 'media_type']\n",
            "\n",
            "Sample message:\n",
            "id: 4358\n",
            "date: 2024-11-23T18:01:42+00:00\n",
            "text: ·çç·äï·âµ·ãç ·àã·àà ·ã®·àç·â•·àµ ·äï·çÉ·âµ ·ã®·àö·åà·àà·åç·àç ·ã®·àç·â•·àµ ·à≥·àô·äì ·àà·àõ·ãò·ãù 0974312223 ·ã≠·ã∞·ãç·àâ¬† ·ãà·ã≠·àù https://t.me/helloo_market_bot?start=131210010 ·ã≠·å†·âÄ·àô! \n",
            "#Madeinethiopia #Detergents #CleaningSplices #Ethiopian #Marketplace #BuyEthiopia\n",
            "views: 3152.0\n",
            "channel: @helloomarketethiopia\n",
            "has_media: True\n",
            "media_type: photo\n"
          ]
        }
      ],
      "source": [
        "# Examine a sample of the raw data\n",
        "if raw_data:\n",
        "    # Convert to DataFrame for easier examination\n",
        "    df_raw = pd.DataFrame(raw_data)\n",
        "    \n",
        "    # Display basic information\n",
        "    print(f\"Number of messages: {len(df_raw)}\")\n",
        "    print(f\"Columns: {df_raw.columns.tolist()}\")\n",
        "    \n",
        "    # Display a sample message\n",
        "    print(\"\\nSample message:\")\n",
        "    sample = df_raw.sample(1).iloc[0]\n",
        "    for key, value in sample.items():\n",
        "        if key == 'text':\n",
        "            # Truncate long text\n",
        "            print(f\"{key}: {value[:200]}...\" if len(value) > 200 else f\"{key}: {value}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value}\")\n",
        "else:\n",
        "    print(\"No data to examine\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing messages: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1045/1045 [00:00<00:00, 1552.70it/s]\n",
            "2025-06-22 10:41:27,285 - src.data.preprocessor - INFO - Saved 1045 processed messages to D:\\10-Academy\\Week4\\amharic-ecommerce-extractor\\data\\processed\\processed_messages.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 1045 messages\n",
            "\n",
            "Sample processed message:\n",
            "Original text: **·â¥·àå·åç·à´·àù****‚≠êÔ∏è**** **https://t.me/nejashionlinemarketing**\n",
            "¬†¬†¬†¬† \" ·àò·àà·ã´·âΩ·äï ·â≥·àõ·äù ·àò·àÜ·äì·âΩ·äï ·å•·à© ·ä•·âÉ ·àõ·âÖ·à®·â£·âΩ·äï üõç ·äê·åÉ·à∫ ...\n",
            "Normalized text: **·â¥·àå·åç·à´·àù****‚≠êÔ∏è**** **https://t.me/nejashionlinemarketing** \" ·àò·àà·ã´·âΩ·äï ·â≥·àõ·äù ·àò·àÜ·äì·âΩ·äï ·å•·à© ·ä•·âÉ ·àõ·âÖ·à®·â£·âΩ·äï üõç ·äê·åÉ·à∫ onlin...\n",
            "Tokens: ['**·â¥·àå·åç·à´·àù****‚≠êÔ∏è***', '*', '**https://t.me/nejashionlinemarketing*', '*', '\"', '·àò·àà·ã´·âΩ·äï', '·â≥·àõ·äù', '·àò·àÜ·äì·âΩ·äï', '·å•·à©', '·ä•·âÉ', '·àõ·âÖ·à®·â£·âΩ·äï', 'üõç', '·äê·åÉ·à∫', 'online', 'SHOP', '\"*', '*', '**KUMTEL', '70', '·àä·âµ·à≠']...\n",
            "\n",
            "Potential entities:\n",
            "prices: ['15,500']\n",
            "locations: []\n"
          ]
        }
      ],
      "source": [
        "# Process the raw data\n",
        "if raw_data:\n",
        "    # Process the data using our AmharicPreprocessor\n",
        "    processed_data = preprocessor.process_data(\n",
        "        raw_data,\n",
        "        output_dir=str(processed_data_dir),\n",
        "        output_filename=\"processed_messages\"\n",
        "    )\n",
        "    \n",
        "    print(f\"Processed {len(processed_data)} messages\")\n",
        "    \n",
        "    # Convert to DataFrame for examination\n",
        "    df_processed = pd.DataFrame(processed_data)\n",
        "    \n",
        "    # Display a sample processed message\n",
        "    print(\"\\nSample processed message:\")\n",
        "    sample = df_processed.sample(1).iloc[0]\n",
        "    \n",
        "    print(f\"Original text: {sample['text'][:100]}...\" if len(sample['text']) > 100 else f\"Original text: {sample['text']}\")\n",
        "    print(f\"Normalized text: {sample['normalized_text'][:100]}...\" if len(sample['normalized_text']) > 100 else f\"Normalized text: {sample['normalized_text']}\")\n",
        "    print(f\"Tokens: {sample['tokens'][:20]}...\" if len(sample['tokens']) > 20 else f\"Tokens: {sample['tokens']}\")\n",
        "    \n",
        "    if 'potential_entities' in sample:\n",
        "        print(\"\\nPotential entities:\")\n",
        "        for entity_type, entities in sample['potential_entities'].items():\n",
        "            print(f\"{entity_type}: {entities}\")\n",
        "else:\n",
        "    print(\"No data to process\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-22 10:41:32,953 - src.data.preprocessor - INFO - Saved NER-ready data to D:\\10-Academy\\Week4\\amharic-ecommerce-extractor\\data\\processed\\ner_ready_data.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NER-ready data sample:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message_id</th>\n",
              "      <th>channel</th>\n",
              "      <th>token</th>\n",
              "      <th>entity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>188877</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>üì¢</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>188877</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·ã≠·àÖ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>188877</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·ã®·â≤·ä≠·â´·àÖ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>188877</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·â¢·ãù·äê·àµ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>188877</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·â§·â∞·à∞·â•</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>188877</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·â§·âµ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>188877</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·äê·ãç</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>188877</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·ç£</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>188877</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·à∞·ãç·äï</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>188877</td>\n",
              "      <td>@tikvahethmart</td>\n",
              "      <td>·ä•·äï·ã≤·àÅ·àù</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  message_id         channel  token entity\n",
              "0     188877  @tikvahethmart      üì¢      O\n",
              "1     188877  @tikvahethmart     ·ã≠·àÖ      O\n",
              "2     188877  @tikvahethmart  ·ã®·â≤·ä≠·â´·àÖ      O\n",
              "3     188877  @tikvahethmart   ·â¢·ãù·äê·àµ      O\n",
              "4     188877  @tikvahethmart   ·â§·â∞·à∞·â•      O\n",
              "5     188877  @tikvahethmart     ·â§·âµ      O\n",
              "6     188877  @tikvahethmart     ·äê·ãç      O\n",
              "7     188877  @tikvahethmart      ·ç£      O\n",
              "8     188877  @tikvahethmart    ·à∞·ãç·äï      O\n",
              "9     188877  @tikvahethmart  ·ä•·äï·ã≤·àÅ·àù      O"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total tokens: 58875\n",
            "Messages: 1046\n",
            "Channels: 9\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for NER labeling\n",
        "if 'processed_data' in locals() and processed_data:\n",
        "    # Prepare data for NER using our AmharicPreprocessor\n",
        "    ner_data = preprocessor.prepare_for_ner(\n",
        "        processed_data,\n",
        "        output_dir=str(processed_data_dir),\n",
        "        output_filename=\"ner_ready_data\"\n",
        "    )\n",
        "    \n",
        "    # Display the first few rows of the NER-ready data\n",
        "    print(\"NER-ready data sample:\")\n",
        "    display(ner_data.head(10))\n",
        "    \n",
        "    # Display statistics\n",
        "    print(f\"\\nTotal tokens: {len(ner_data)}\")\n",
        "    print(f\"Messages: {ner_data['message_id'].nunique()}\")\n",
        "    print(f\"Channels: {ner_data['channel'].nunique()}\")\n",
        "else:\n",
        "    print(\"No processed data available for NER preparation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No processed data available\n"
          ]
        }
      ],
      "source": [
        "# Helper function to visualize entity patterns\n",
        "def visualize_entity_patterns():\n",
        "    \"\"\"\n",
        "    Visualize common patterns for potential entities in the data.\n",
        "    \"\"\"\n",
        "    if 'df_processed' not in locals() or len(df_processed) == 0:\n",
        "        print(\"No processed data available\")\n",
        "        return\n",
        "    \n",
        "    # Extract price patterns\n",
        "    price_pattern = r'(\\d+(?:,\\d+)*(?:\\.\\d+)?)\\s*(?:·â•·à≠|ETB|Birr|birr)'\n",
        "    prices = []\n",
        "    \n",
        "    for text in df_processed['normalized_text']:\n",
        "        matches = re.findall(price_pattern, text, re.IGNORECASE)\n",
        "        prices.extend(matches)\n",
        "    \n",
        "    print(f\"Found {len(prices)} price mentions\")\n",
        "    print(f\"Sample prices: {prices[:10]}\")\n",
        "    \n",
        "    # Count location mentions\n",
        "    locations = []\n",
        "    for row in df_processed.iterrows():\n",
        "        if 'potential_entities' in row[1] and 'locations' in row[1]['potential_entities']:\n",
        "            locations.extend(row[1]['potential_entities']['locations'])\n",
        "    \n",
        "    location_counts = pd.Series(locations).value_counts()\n",
        "    print(\"\\nLocation mentions:\")\n",
        "    print(location_counts.head(10))\n",
        "\n",
        "# Visualize entity patterns\n",
        "visualize_entity_patterns()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "In this notebook, we have:\n",
        "\n",
        "1. Loaded raw data collected from Telegram channels\n",
        "2. Cleaned and normalized the Amharic text\n",
        "3. Tokenized the text for NER tasks\n",
        "4. Extracted potential entities using pattern matching\n",
        "5. Prepared the data in a format suitable for NER labeling\n",
        "\n",
        "The preprocessed data has been saved to:\n",
        "- `processed_messages.json`: Contains the full processed messages with normalized text and potential entities\n",
        "- `ner_ready_data.csv`: Contains tokenized data ready for NER labeling\n",
        "\n",
        "In the next notebook, we will manually label a subset of this data for NER training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
